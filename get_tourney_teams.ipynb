{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Eliminate non-tournament teams"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seeds = pd.read_csv(\"data/tourney_seeds.csv\")\n",
      "scores = pd.read_csv(\"data/regular_season_results.csv\")\n",
      "all_actual_scores = pd.Series(dtype='int')\n",
      "all_pred_scores = pd.Series(dtype='float64')\n",
      "\n",
      "# test with all the seasons\n",
      "seasons  = map(chr, range(ord('A'), ord('R') + 1)) # iterate from season 'A' to 'R'\n",
      "accuracy_array = np.array([0,0]) # stores [accurate_predictions, num_predictions]\n",
      "x_hat_list = []\n",
      "\n",
      "for season in seasons:\n",
      "    teams = np.array(seeds[seeds['season'] == season]['team']) # easier to work with numpy array than panda series\n",
      "    TEST_SIZE = 100\n",
      "    \n",
      "    # # for testing purposes, make this list much smaller\n",
      "    # teams = teams[0:9]\n",
      "    # TEST_SIZE = 3 # also, make the test size really small\n",
      "    \n",
      "    # record the matrix index for each team\n",
      "    team_dict = {}\n",
      "    for i in range(len(teams)):\n",
      "        teams[i]\n",
      "        team_dict[ teams[i] ] = i\n",
      "    \n",
      "    # temp, just season scores\n",
      "    temp = scores[scores['season'] == season]\n",
      "    if_in_team_dict = np.repeat(True, len(temp))\n",
      "    for i in range(len(if_in_team_dict)):\n",
      "        if_in_team_dict[i] = temp['wteam'].iloc[i] in team_dict and temp['lteam'].iloc[i] in team_dict\n",
      "    \n",
      "    season_scores = temp[if_in_team_dict]\n",
      "    # split train season scores and test season scores\n",
      "    train_size = len(season_scores) - TEST_SIZE\n",
      "    train = season_scores.iloc[0 : train_size,]\n",
      "    test = season_scores.iloc[train_size: ,]\n",
      "    \n",
      "    # game incidence matrix (winner = +1, loser = -1)\n",
      "    G = np.zeros([len(train), len(teams)])\n",
      "    \n",
      "    # for each row\n",
      "    for i in range( len(train) ):\n",
      "        # index for winning/losing team\n",
      "        row = train.iloc[i,]\n",
      "        i_win = team_dict[ row['wteam'] ]\n",
      "        i_lose = team_dict[ row['lteam'] ]\n",
      "        G[i, i_win] = 1; G[i, i_lose] = -1\n",
      "        \n",
      "    train['score_diffs'] = train['wscore'] - train['lscore']\n",
      "    \n",
      "    x_hat = np.linalg.lstsq(G, np.array(train['score_diffs']))[0]\n",
      "    x_hat_list.append(x_hat)\n",
      "    \n",
      "    # input the guess score difference\n",
      "    wteam_potential = test['wteam'].map(lambda t: x_hat[ team_dict[t] ])\n",
      "    lteam_potential = test['lteam'].map(lambda t: x_hat[ team_dict[t] ])\n",
      "    test['pred_scores'] = wteam_potential - lteam_potential\n",
      "    all_pred_scores = pd.concat([ all_pred_scores, test['pred_scores'] ])\n",
      "    all_actual_scores = pd.concat([ all_actual_scores, test['wscore'] - test['lscore'] ])\n",
      "    test['prediction'] = test['pred_scores'] > 0 # prediction that the 'wteam' will win (if True, then this is accurate)\n",
      "\n",
      "    \n",
      "    # add to the accuracy array ([num_correct_predictions, num_precitions])\n",
      "    accuracy_array += np.array([ sum(test['prediction']), len(test['prediction']) ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 161
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_estimate = float(accuracy_array[0]) / accuracy_array[1]\n",
      "\n",
      "# CI = confidence interval\n",
      "CI_length = 1.3 / np.sqrt( accuracy_array[1] ) \n",
      "# 99% CI ([lower_bound, upper_bound])\n",
      "CI_99_percent = np.array([accuracy_estimate - CI_length, accuracy_estimate + CI_length])\n",
      "CI_99_percent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 162,
       "text": [
        "array([ 0.50380315,  0.56508574])"
       ]
      }
     ],
     "prompt_number": 162
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wteam_potential = train['wteam'].map(lambda t: x_hat[ team_dict[t] ])\n",
      "lteam_potential = train['lteam'].map(lambda t: x_hat[ team_dict[t] ])\n",
      "train['pred_scores'] = wteam_potential - lteam_potential\n",
      "#plt.ylim(-50, 50) # force the y-domain\n",
      "train['prediction'] = wteam_potential - lteam_potential > 0 # prediction that the 'wteam' will win (if True, then this is accurate)\n",
      "\n",
      "np.corrcoef(all_actual_scores, all_pred_scores)[0][1]\n",
      "# plt.scatter(all_actual_scores, all_pred_scores)\n",
      "# plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 191,
       "text": [
        "0.067184855403095156"
       ]
      }
     ],
     "prompt_number": 191
    }
   ],
   "metadata": {}
  }
 ]
}