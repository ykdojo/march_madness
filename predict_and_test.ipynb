{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn import linear_model\n",
      "from time import time\n",
      "results = pd.read_csv(\"data/tourney_results.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test the prediction method with all the seasons\n",
      "seeds = pd.read_csv(\"data/tourney_seeds.csv\")\n",
      "scores = pd.read_csv(\"data/regular_season_results.csv\")\n",
      "results = pd.read_csv(\"data/tourney_results.csv\")\n",
      "\n",
      "# test with different values of lambda, the regularization parameter\n",
      "HYPER_PARAMETERS = [0, 10 ** (-12), 10**(-9), 10**(-6), 10**(-3)]\n",
      "list_of_x_hat_lists = []\n",
      "list_of_accuracy_arrays = []\n",
      "list_of_all_actual_scores = []\n",
      "list_of_all_pred_scores = []\n",
      "\n",
      "\n",
      "# test with all the seasons\n",
      "seasons  = map(chr, range(ord('A'), ord('R') + 1))\n",
      "\n",
      "t0 = time() # benchmark the 18 iterations\n",
      "# for all hyper parameters\n",
      "for hp in HYPER_PARAMETERS:\n",
      "    all_actual_scores = pd.Series(dtype='int')\n",
      "    all_pred_scores = pd.Series(dtype='float64')\n",
      "    x_hat_list = []\n",
      "    accuracy_array = np.array([0,0]) # stores [accurate_predictions, num_predictions]\n",
      "    for season in seasons: # for all seasons\n",
      "        #teams = np.array(seeds[seeds['season'] == season]['team']) # easier to work with numpy array than panda series\n",
      "        teams = xrange(501, 857) # TODO: DOING NOW: use ALL the teams for this calculation\n",
      "        \n",
      "        # FOR DEBUGGING, just use 10 teams to speed up the computation.\n",
      "        #teams = teams[0:10]\n",
      "        \n",
      "        # record the matrix index for each team\n",
      "        team_dict = {}\n",
      "        for i in range(len(teams)):\n",
      "            teams[i]\n",
      "            team_dict[ teams[i] ] = i\n",
      "        \n",
      "        # Just get the current season\n",
      "        current = scores[scores['season'] == season]\n",
      "        current_results = results[results['season'] == season]\n",
      "        \n",
      "        # The following is useful if we have a limited number of teams in our model\n",
      "        if_in_team_dict = np.repeat(True, len(current))\n",
      "        if_in_team_dict_test = np.repeat(True, len(current_results))\n",
      "        for i in range(len(if_in_team_dict)):\n",
      "            if_in_team_dict[i] = current['wteam'].iloc[i] in team_dict and current['lteam'].iloc[i] in team_dict\n",
      "        for i in range(len(if_in_team_dict_test)):\n",
      "            if_in_team_dict_test[i] = current_results['wteam'].iloc[i] in team_dict and current_results['lteam'].iloc[i] in team_dict\n",
      "\n",
      "        train = current[if_in_team_dict]\n",
      "        test = current_results[if_in_team_dict_test]\n",
      "        \n",
      "        # game incidence matrix (winner = +1, loser = -1)\n",
      "        G = np.zeros([len(train), len(teams)])\n",
      "        \n",
      "        # for each row\n",
      "        for i in range( len(train) ):\n",
      "            # index for winning/losing team\n",
      "            row = train.iloc[i,]\n",
      "            i_win = team_dict[ row['wteam'] ]\n",
      "            i_lose = team_dict[ row['lteam'] ]\n",
      "            G[i, i_win] = 1; G[i, i_lose] = -1\n",
      "            \n",
      "        train['score_diffs'] = train['wscore'] - train['lscore']\n",
      "     \n",
      "        if hp == 0:\n",
      "            x_hat = np.linalg.lstsq(G, np.array(train['score_diffs']))[0]\n",
      "        else:\n",
      "            clf = linear_model.Ridge (alpha = hp)\n",
      "            x_hat = clf.fit(G, train['score_diffs']).coef_\n",
      "\n",
      "        x_hat_list.append(x_hat)\n",
      "        \n",
      "        # input the guess score difference\n",
      "        wteam_potential = test['wteam'].map(lambda t: x_hat[ team_dict[t] ])\n",
      "        lteam_potential = test['lteam'].map(lambda t: x_hat[ team_dict[t] ])\n",
      "        test['pred_scores'] = wteam_potential - lteam_potential\n",
      "        all_pred_scores = pd.concat([ all_pred_scores, test['pred_scores'] ])\n",
      "        all_actual_scores = pd.concat([ all_actual_scores, test['wscore'] - test['lscore'] ])\n",
      "        test['prediction'] = test['pred_scores'] > 0 # prediction that the 'wteam' will win (if True, then this is accurate)\n",
      "    \n",
      "        \n",
      "        # add to the accuracy array ([num_correct_predictions, num_precitions])\n",
      "        accuracy_array += np.array([ sum(test['prediction']), len(test['prediction']) ])\n",
      "    list_of_x_hat_lists.append(x_hat_list)\n",
      "    list_of_accuracy_arrays.append(accuracy_array)\n",
      "    list_of_all_actual_scores.append(all_actual_scores)\n",
      "    list_of_all_pred_scores.append(all_pred_scores)\n",
      "    \n",
      "time_passed = time() - t0\n",
      "print \"time passed\", time_passed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(HYPER_PARAMETERS)):\n",
      "    accuracy_estimate = float(list_of_accuracy_arrays[i][0]) / list_of_accuracy_arrays[i][1]\n",
      "\n",
      "    # CI = confidence interval\n",
      "    CI_length = 1.3 / np.sqrt( list_of_accuracy_arrays[i][1] )\n",
      "    # 99% CI ([lower_bound, upper_bound])\n",
      "    CI_99_percent = np.array([accuracy_estimate - CI_length, accuracy_estimate + CI_length])\n",
      "    \n",
      "    x_hat_list = list_of_x_hat_lists[i]\n",
      "    s = 0\n",
      "    for x in x_hat_list:\n",
      "        s += np.std(x)\n",
      "    average_std = s / len(x_hat_list)\n",
      "\n",
      "    print \"hyperparameter: \", HYPER_PARAMETERS[i]\n",
      "    print \"accuracy (99% CI): \", CI_99_percent\n",
      "    print \"correlation coefficient:\", np.corrcoef(list_of_all_actual_scores[i], list_of_all_pred_scores[i])[0][1]\n",
      "    print \"max, min (pred):\", list_of_all_pred_scores[i].max(), \",\", list_of_all_pred_scores[i].min()\n",
      "    print \"average std: \", average_std\n",
      "    print \"===============\"\n",
      "    #print \"max, min (actual):\", all_actual_scores.max(), \",\", all_actual_scores.min()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hyperparameter:  0\n",
        "accuracy (99% CI):  [ 0.66418685  0.74065744]\n",
        "correlation coefficient: 0.456181374908\n",
        "max, min (pred): 62.515625 , -47.9375\n",
        "average std:  1.35054261064e+15\n",
        "===============\n",
        "hyperparameter:  1e-12\n",
        "accuracy (99% CI):  [ 0.65726644  0.73373702]\n",
        "correlation coefficient: 0.440604518715\n",
        "max, min (pred): 62.515625 , -47.9375\n",
        "average std:  5.75801209289\n",
        "===============\n",
        "hyperparameter:  1e-09\n",
        "accuracy (99% CI):  [ 0.65726644  0.73373702]\n",
        "correlation coefficient: 0.441107035784\n",
        "max, min (pred): 62.515625 , -47.9375\n",
        "average std:  5.75794135894\n",
        "===============\n",
        "hyperparameter:  1e-06\n",
        "accuracy (99% CI):  [ 0.65726644  0.73373702]\n",
        "correlation coefficient: 0.443534320014\n",
        "max, min (pred): 62.515625 , -47.9375\n",
        "average std:  5.75794073682\n",
        "===============\n",
        "hyperparameter:  0.001\n",
        "accuracy (99% CI):  [ 0.65726644  0.73373702]\n",
        "correlation coefficient: 0.445988828425\n",
        "max, min (pred): 62.515625 , -47.9375\n",
        "average std:  5.75731877644\n",
        "===============\n"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_hat_list = list_of_x_hat_lists[2]\n",
      "s = 0\n",
      "for x in x_hat_list:\n",
      "    s += np.std(x)\n",
      "average_std = s / len(x_hat_list)\n",
      "average_std"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 134,
       "text": [
        "5.7579413589382105"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(list_of_all_actual_scores[0], list_of_all_pred_scores[0])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# errors = all_actual_scores - all_pred_scores\n",
      "# errors = all_actual_scores\n",
      "# errors_shuffle = np.zeros(len(errors))\n",
      "# for i in range(len(errors)):\n",
      "#     if np.random.rand() > 0.5:\n",
      "#         errors_shuffle[i] = -errors[i]\n",
      "#     else:\n",
      "#         errors_shuffle[i] = errors[i]\n",
      "# plt.hist(errors_shuffle, bins=30)\n",
      "# plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    }
   ],
   "metadata": {}
  }
 ]
}