{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn import linear_model\n",
      "from time import time\n",
      "results = pd.read_csv(\"data/tourney_results.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test the prediction method with all the seasons\n",
      "seeds = pd.read_csv(\"data/tourney_seeds.csv\")\n",
      "scores = pd.read_csv(\"data/regular_season_results.csv\")\n",
      "results = pd.read_csv(\"data/tourney_results.csv\")\n",
      "\n",
      "# test with different values of lambda, the regularization parameter\n",
      "HYPER_PARAMETERS = [0, 10 ** (-12), 10**(-9), 10**(-3)]\n",
      "list_of_x_hat_lists = []\n",
      "list_of_accuracy_arrays = []\n",
      "list_of_all_actual_scores = []\n",
      "list_of_all_pred_scores = []\n",
      "\n",
      "# test with all the seasons\n",
      "seasons  = map(chr, range(ord('A'), ord('S') + 1))\n",
      "\n",
      "t0 = time() # benchmark the 18 iterations\n",
      "# for all hyper parameters\n",
      "for hp in HYPER_PARAMETERS:\n",
      "    all_actual_scores = pd.Series(dtype='int')\n",
      "    all_pred_scores = pd.Series(dtype='float64')\n",
      "    x_hat_list = []\n",
      "    accuracy_array = np.array([0,0]) # stores [accurate_predictions, num_predictions]\n",
      "    for season in seasons: # for all seasons\n",
      "        #teams = np.array(seeds[seeds['season'] == season]['team']) # easier to work with numpy array than panda series\n",
      "        teams = xrange(501, 857) # TODO: DOING NOW: use ALL the teams for this calculation\n",
      "        \n",
      "        # FOR DEBUGGING, just use 10 teams to speed up the computation.\n",
      "        #teams = teams[0:10]\n",
      "        \n",
      "        # record the matrix index for each team\n",
      "        team_dict = {}\n",
      "        for i in range(len(teams)):\n",
      "            teams[i]\n",
      "            team_dict[ teams[i] ] = i\n",
      "        \n",
      "        # Just get the current season\n",
      "        current = scores[scores['season'] == season]\n",
      "        current_results = results[results['season'] == season]\n",
      "        \n",
      "        # The following is useful if we have a limited number of teams in our model\n",
      "        if_in_team_dict = np.repeat(True, len(current))\n",
      "        if_in_team_dict_test = np.repeat(True, len(current_results))\n",
      "        for i in range(len(if_in_team_dict)):\n",
      "            if_in_team_dict[i] = current['wteam'].iloc[i] in team_dict and current['lteam'].iloc[i] in team_dict\n",
      "        for i in range(len(if_in_team_dict_test)):\n",
      "            if_in_team_dict_test[i] = current_results['wteam'].iloc[i] in team_dict and current_results['lteam'].iloc[i] in team_dict\n",
      "\n",
      "        train = current[if_in_team_dict]\n",
      "        test = current_results[if_in_team_dict_test]\n",
      "        \n",
      "        # game incidence matrix (winner = +1, loser = -1)\n",
      "        G = np.zeros([len(train), len(teams)])\n",
      "        \n",
      "        # for each row\n",
      "        for i in range( len(train) ):\n",
      "            # index for winning/losing team\n",
      "            row = train.iloc[i,]\n",
      "            i_win = team_dict[ row['wteam'] ]\n",
      "            i_lose = team_dict[ row['lteam'] ]\n",
      "            G[i, i_win] = 1; G[i, i_lose] = -1\n",
      "            \n",
      "        train['score_diffs'] = train['wscore'] - train['lscore']\n",
      "     \n",
      "        if hp == 0:\n",
      "            x_hat = np.linalg.lstsq(G, np.array(train['score_diffs']))[0]\n",
      "        else:\n",
      "            clf = linear_model.Ridge (alpha = hp)\n",
      "            x_hat = clf.fit(G, train['score_diffs']).coef_\n",
      "\n",
      "        x_hat_list.append(x_hat)\n",
      "        \n",
      "        # input the guess score difference\n",
      "        wteam_potential = test['wteam'].map(lambda t: x_hat[ team_dict[t] ])\n",
      "        lteam_potential = test['lteam'].map(lambda t: x_hat[ team_dict[t] ])\n",
      "        test['pred_scores'] = wteam_potential - lteam_potential\n",
      "        all_pred_scores = pd.concat([ all_pred_scores, test['pred_scores'] ])\n",
      "        all_actual_scores = pd.concat([ all_actual_scores, test['wscore'] - test['lscore'] ])\n",
      "        test['prediction'] = test['pred_scores'] > 0 # prediction that the 'wteam' will win (if True, then this is accurate)\n",
      "    \n",
      "        \n",
      "        # add to the accuracy array ([num_correct_predictions, num_precitions])\n",
      "        accuracy_array += np.array([ sum(test['prediction']), len(test['prediction']) ])\n",
      "    list_of_x_hat_lists.append(x_hat_list)\n",
      "    list_of_accuracy_arrays.append(accuracy_array)\n",
      "    list_of_all_actual_scores.append(all_actual_scores)\n",
      "    list_of_all_pred_scores.append(all_pred_scores)\n",
      "    \n",
      "time_passed = time() - t0\n",
      "print \"time passed\", time_passed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "time passed 81.5909349918\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(HYPER_PARAMETERS)):\n",
      "    accuracy_estimate = float(list_of_accuracy_arrays[i][0]) / list_of_accuracy_arrays[i][1]\n",
      "\n",
      "    # CI = confidence interval\n",
      "    CI_length = 1.3 / np.sqrt( list_of_accuracy_arrays[i][1] )\n",
      "    # 99% CI ([lower_bound, upper_bound])\n",
      "    CI_99_percent = np.array([accuracy_estimate - CI_length, accuracy_estimate + CI_length])\n",
      "    \n",
      "    x_hat_list = list_of_x_hat_lists[i]\n",
      "    s = 0\n",
      "    for x in x_hat_list:\n",
      "        s += np.std(x)\n",
      "    average_std = s / len(x_hat_list)\n",
      "\n",
      "    print \"hyperparameter: \", HYPER_PARAMETERS[i]\n",
      "    print \"accuracy (99% CI): \", CI_99_percent\n",
      "    print \"correlation coefficient:\", np.corrcoef(list_of_all_actual_scores[i], list_of_all_pred_scores[i])[0][1]\n",
      "    print \"max, min (pred):\", list_of_all_pred_scores[i].max(), \",\", list_of_all_pred_scores[i].min()\n",
      "    print \"average std: \", average_std\n",
      "    print \"scores_length (actual, pred)\", len(list_of_all_actual_scores[i]), \",\", len(list_of_all_pred_scores[i])\n",
      "    print \"===============\"\n",
      "    #print \"max, min (actual):\", all_actual_scores.max(), \",\", all_actual_scores.min()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hyperparameter:  0\n",
        "accuracy (99% CI):  [ 0.66418685  0.74065744]\n",
        "correlation coefficient: 0.456181374908\n",
        "max, min (pred): 62.515625 , -47.9375\n",
        "average std:  1.30235599934e+15\n",
        "scores_length (actual, pred) 1156 , 1156\n",
        "===============\n",
        "hyperparameter:  1e-12\n",
        "accuracy (99% CI):  [ 0.65726644  0.73373702]\n",
        "correlation coefficient: 0.468418104994\n",
        "max, min (pred): 30.817340342 , -17.4129734984\n",
        "average std:  5.74449236876\n",
        "scores_length (actual, pred) 1156 , 1156\n",
        "===============\n",
        "hyperparameter:  1e-09\n",
        "accuracy (99% CI):  [ 0.65726644  0.73373702]\n",
        "correlation coefficient: 0.46841810499\n",
        "max, min (pred): 30.817340339 , -17.4129734969\n",
        "average std:  5.74442518922\n",
        "scores_length (actual, pred) 1156 , 1156\n",
        "===============\n",
        "hyperparameter:  0.001\n",
        "accuracy (99% CI):  [ 0.65726644  0.73373702]\n",
        "correlation coefficient: 0.468413856188\n",
        "max, min (pred): 30.8143353615 , -17.4115298304\n",
        "average std:  5.74380634363\n",
        "scores_length (actual, pred) 1156 , 1156\n",
        "===============\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.subplot(221)\n",
      "lim = [-60,60]\n",
      "plt.ylim(lim)\n",
      "\n",
      "i = 0\n",
      "plt.title(\"regularization parameter = \" + str(HYPER_PARAMETERS[i]))\n",
      "plt.scatter(list_of_all_actual_scores[i], list_of_all_pred_scores[i], alpha=0.5, color='b')\n",
      "plt.xlabel(\"actual score differences\")\n",
      "plt.ylabel(\"predicted score differences\")\n",
      "\n",
      "plt.subplot(222)\n",
      "plt.ylim(lim)\n",
      "i = 1\n",
      "plt.title(\"regularization parameter = \" + str(HYPER_PARAMETERS[i]))\n",
      "plt.scatter(list_of_all_actual_scores[i], list_of_all_pred_scores[i], alpha=0.5, color='r')\n",
      "plt.xlabel(\"actual score differences\")\n",
      "plt.ylabel(\"predicted score differences\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.subplot(223)\n",
      "plt.ylim(lim)\n",
      "i = 2\n",
      "plt.title(\"regularization parameter = \" + str(HYPER_PARAMETERS[i]))\n",
      "plt.scatter(list_of_all_actual_scores[i], list_of_all_pred_scores[i], alpha=0.5, color='y')\n",
      "plt.xlabel(\"actual score differences\")\n",
      "plt.ylabel(\"predicted score differences\")\n",
      "\n",
      "plt.subplot(224)\n",
      "plt.ylim(lim)\n",
      "i = 3\n",
      "plt.title(\"regularization parameter = \" + str(HYPER_PARAMETERS[i]))\n",
      "plt.scatter(list_of_all_actual_scores[i], list_of_all_pred_scores[i], alpha=0.5, color='g')\n",
      "plt.xlabel(\"actual score differences\")\n",
      "plt.ylabel(\"predicted score differences\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fit a logistic regression model\n",
      "# Flip the scores so that the regression model recognizes the two classes (True and False)\n",
      "HYPER_INDEX = 0 #indexes the hyper parameter\n",
      "pred_scores = np.array(list_of_all_pred_scores[HYPER_INDEX]).reshape(len(list_of_all_pred_scores[HYPER_INDEX]), 1)\n",
      "pred_scores[-1] = -pred_scores[-1]\n",
      "y = np.repeat(True, len(pred_scores))\n",
      "y[-1] = False\n",
      "\n",
      "model = linear_model.LogisticRegression(fit_intercept=False)\n",
      "model.fit(pred_scores, y)\n",
      "model.coef_[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 111,
       "text": [
        "0.095513338179680649"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sub_dat = pd.read_csv(\"data/sample_submission.csv\")\n",
      "\n",
      "# stores the prediction probabilities\n",
      "pred = np.zeros( len(sub_dat) )\n",
      "sub_all_pred_score = []\n",
      "# for each row\n",
      "for i in range( len(sub_dat) ):\n",
      "    season, team1, team2 = sub_dat.id[i].split('_')\n",
      "    season_index = ord(season) - ord('A')\n",
      "    t1_index = team_dict[int(team1)] # I can do this because team_dict is common for all the seasons\n",
      "    t2_index = team_dict[int(team2)]\n",
      "    x_hat = list_of_x_hat_lists[HYPER_INDEX][season_index]\n",
      "    pred_score =  x_hat[t1_index] - x_hat[t2_index]\n",
      "    sub_all_pred_score.append(pred_score)\n",
      "    pred[i] = model.predict_proba([pred_score])[0][1]\n",
      "sub_dat.pred = pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.array(sub_all_pred_score).max()\n",
      "print np.array(sub_all_pred_score).min()\n",
      "\n",
      "print np.array(pred).max()\n",
      "print np.array(pred).min()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "32.94921875\n",
        "-31.30859375\n",
        "0.95879390144\n",
        "0.0478619913113\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist(pred, bins=30)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sub_dat.to_csv(\"my_submission/real_reg_e-12.csv\", index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"average score for all the tournaments: \", (results['wscore'].mean() + results['lscore'].mean()) / 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "average score for all the tournaments:  69.6085640138\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Print all the estimated potentials for teams, regularized version and non-regularized version.\n",
      "SEASON = 'S'\n",
      "df = pd.read_csv(\"data/teams.csv\")\n",
      "df['estimated_potential_no_reg'] = df.id.map(lambda id: list_of_x_hat_lists[0][ord(SEASON) - ord('A')][team_dict[id]])\n",
      "df['estimated_potential_no_reg'] = df['estimated_potential_no_reg'] - np.max(df['estimated_potential_no_reg'])\n",
      "df['estimated_potential_with_reg'] = df.id.map(lambda id: list_of_x_hat_lists[1][ord(SEASON) - ord('A')][team_dict[id]])\n",
      "df = df.sort([\"estimated_potential_no_reg\", \"estimated_potential_with_reg\"], ascending=False)\n",
      "df['rank'] = range(0, len(df))\n",
      "df.to_csv(\"my_submission/potential_list.csv\", index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    }
   ],
   "metadata": {}
  }
 ]
}